Setup Instructions
==============================

The process requires the following steps:
- Create AWS Accounts
- Configure Default Network
- Setup Source Control
- Connect To VPC
- Setup Database
- Connect MySQL Workbench
- Restore Database From Backup
- Create Database Users
- Setup Secrets Manager
- Install Command Line Tools
- Setup CDN Bucket
- Setup HTTPS Certificate
- Setup Fargate Application Cluster
- Setup DNS Records
- Debug Cluster
- Configure SES
- Configure SNS
- Restore Thelander Platform from backup
- References


Create AWS Accounts
-------------------

Accounts will be created for development, staging, and production.

The accounts will have a name which is the generic email:
staging.{client}@{client domain}
or
staging.{product name}@{client domain}

To start, the account will have just a name and a billing credit card.


Configure Default Network
-------------------------

When an AWS account is created a default VPC and subnets are created.

The default VPC network is a Class B network /16 and AWS will create a /20 subnet for each availability zone. There can be a total of 16 /20 networks in a /16 network with starting addresses per the table generated by:
https://www.site24x7.com/tools/ipv4-subnetcalculator.html
Network Address Block: 172.31.0.0/16
Subnet Mask: 255.255.240.0/20

To make sure the VPN is not affected by changes to the CloudFormation scripts create a subnet 255.255.240.0 named "vpn" which will be used by the VPN server. Select the subnet using the checkbox in the left of the subnet list and use the "Actions" button and select "Modify auto-assign IP settings", select the checkbox, and "Save".

The cloudformation stack requires at least 4 subnets with 2 used by the application load balancer and 2 used by lambda functions to access the VPC from outside the VPC context. Therefore, if you are running in a AWS Region (Geo) which has less than 4 Availability Zones add additional /20 subnets. Distribute the additional subnets across the Availability Zones (AZs) so both the load balancer and the lambdas have redundancy.

Type the VPC and subnets into the respective development, staging, and production parameter files in the parameters/ directory.


Setup Source Control
--------------------

Connect to your GitHub repository or create one.

Select the repo to use or create one with the name {project name}.

The URL in the address line of the browser will read:
https://github.com/{project owner}/{project name}

cd to the root directory for the project.

Setup git to push to remote:
yum -y install ca-certificates
# Notice the next command does have {project owner} twice in the path.
git init
git remote add {project name} git@github.com:{project owner}/{project name}.git

Verify the remote is set correctly.
git remote -v

Add any files to the repo:
git add .

Remove any files from the repo:
git add -u

Do the initial checkin:
git commit -a -m "Initial checkin."

Generate a GitHub OAuth access token:
1) Log into GitHub account (repo owner) which has the {repo project}.
2) Select "Settings"
3) Select "Developer Settings"
4) Select "Personal access tokens"
5) Select "Generate new token"
6) Name the token as {project name}-codePipeline
7) Select the top level "repo" and top level "admin:repo_hook"
8) Select "Generate token"
9) Grab the token which will NOT be shown again and put into the master password database. We will refer to is as {repo token}


Connect To VPC
--------------

[[TODO: Suz write process here for setup of openVPN server and creating team credentials.]]


Setup Database
--------------

Substitute {project name} and {development phase} removing the curly braces.

Create parameter group to allow stored procedures:
- AWS RDS
- If no dataqbases there will be no menu to the left use the hamburger menu to see the detailed menu on the left.
- Select "Parameter Groups"
- Select "Create Parameter Group"
- Base on "Aurora5.6"
- Select "DB Parameter Group"
- Name "aurora5-6-stored-procedures"
- Description "Allow stored procedures."
- Look for parameter "log_bin" and select log_bin_trust_function_creators
- Select edit and set to 1 per this article:
https://www.quora.com/How-can-I-create-stored-procedures-for-AWS-RDS-MySQL
- Save changes.
NOTE: Have not found a way to associate the "Parameter group" (not a "DB cluster parameter group") to a database but restore of the database backup works including loading the stored procedures. Therefore, it seems that just having the group exist has it associated with all databases.

Setup RDS Aurora
- AWS RDS
- Select "Create database"
- Standard Create
- Serverless
- Select Regional
- Aurora and Database: 5.6.10a
- One writer and multiple readers
- Cluster Name: {project name}-cluster
- Master Username: admin
WARNING: Do NOT use a password with symbols because AWS Secrets Manager is restrictive. If you do you will need to use this command to change the password later since others will not work:
UPDATE mysql.user SET Password=PASSWORD('{strong password with no symbols}') WHERE USER='masterUser' AND Host='%';
- Burstable images.
- Create Aurora
- Default VPC
- Not publically accessible
- Default cluster identifier
- Select db and cluster groups created above
- Create cluster
- Select the "Endpoint" Arn from the console which will now be refered to as {database endpoint} below.

Enable Security Group Access:
- RDS will create a security group automatically related to the database cluster which has a link on the cluster detail page or can be found under "Security Groups" in the EC2 console.
- By default it is given the name "rds-launch-wizard".
- Make sure this security group has an "Inbound" rule on TCP 3306 with the source being the full VPC range. The VPC range can be found by going the AWS VPC, select teh VPC used, and put in the value of the IPv4 CIDR that will be in the form 172.31.0.0/16 or similar.
It appears AWS RDS creates the default rule with the routable IP of the machine which created the database cluster.

Connect to the database from the command line:
mysql -u masterUser -p{database password} -h {database endpoint}


Connect MySQL Workbench
-----------------------

- Select Home tab
- Select "+" to create a connection
- Name: {project name}-{development phase}-cluster
- Connection method: Standard TCP/IP
- Endpoint extracted from AWS Console: {database endpoint}
- Use your developer user, appUser for testing, and avoid using the masterUser.
- Use the "Test" button to text the connection


Restore Database From Backup
----------------------------

Download sqldump backup from S3.

Or, if you create with mysqldump:
mysqldump -u admin -p{database password no space} -h {database endpoint} --databases {database name} --column-statistics=0 > {database backup file}.sql

Restore the database if required by downloading a mysqldump file and running the following line from the command line:
gunzip < {database backup file}.sql.gz | sed 's/\sDEFINER=`[^`]*`@`[^`]*`//g' | mysql -u {database master user login} -p{database password - no space} -h {database endpoint}

If you get the error:
Access denied; you need (at least one of) the SUPER privilege(s) for this operation
Some backups have a DEFINER which must be removed so a line like this:
CREATE DEFINER=`root`@`localhost` FUNCTION `fn_name`(
Will become this:
CREATE FUNCTION `fn_name`(


Create Database Users
---------------------

Create database user which will be used by the application server to access the database:

When granting privilages *.* represents database.table and means all tables of all databases.

One can see which users are in the database using:
SELECT User, Host, Password FROM mysql.user;

Create a user to be used by the application server with limited privilages:
CREATE USER 'appUser' IDENTIFIED BY '{strong password}';
-- NOTE: This must be done for EACH database!
GRANT SELECT, INSERT, UPDATE, CREATE TEMPORARY TABLES, EXECUTE ON `{database name}`.* TO 'appUser';
FLUSH PRIVILEGES;

To give access to multiple database use:
GRANT SELECT, INSERT, UPDATE, CREATE TEMPORARY TABLES, EXECUTE ON `thelander_transactional`.* TO 'appUser';
GRANT SELECT, INSERT, UPDATE, CREATE TEMPORARY TABLES, EXECUTE ON `thelander_analytical`.* TO 'appUser';
FLUSH PRIVILEGES;

Additionally, the global privilage which must be granted *.* to allow serverless-mysql to scale:
GRANT PROCESS ON *.* TO 'appUser';
FLUSH PRIVILEGES;

One can see the user was added:
SELECT User, Host, Password FROM mysql.user;

The definitions for all the privilages is here:
https://dev.mysql.com/doc/refman/8.0/en/grant.html

Verify the privilages were granted:
SHOW GRANTS FOR appUser;

Use this SQL to add a developer:
CREATE USER '{developer name}' IDENTIFIED BY '{strong password}';
GRANT SELECT,INSERT, UPDATE, DELETE, CREATE, DROP, RELOAD, PROCESS, REFERENCES, INDEX, ALTER, SHOW DATABASES, CREATE TEMPORARY TABLES, EXECUTE, CREATE ROUTINE, ALTER ROUTINE, EVENT, TRIGGER ON *.* TO '{developer name}';
FLUSH PRIVILEGES;

Updating the user password depends on the version of mySQL. For 5.6 use:
SET PASSWORD FOR 'user-name'@'localhost' = PASSWORD('NEW_USER_PASSWORD');
FLUSH PRIVILEGES;
Referenced in this article:
https://linuxize.com/post/how-to-change-mysql-user-password/
Must do the reset of the appUser password on restore of a database.


Setup Secrets Manager
---------------------

Database passwords are kept in AWS Secrets Manager and the nodejs application server will have IAM rights to pull them at runtime.

Open AWS Secrets Manager.

WARNING: MySQL has a limit of 15 character database user names. And, when AWS does a password rotation on the master user it appends more characters onto the name. Therefore, use the short name created by RDS.

WARNING: When one is asked to 'use' or 'create' a lambda function for password rotation the lambda function used by the administrative user who uses its own password to change itself is a different lambda than the one which is used for the application password which uses the administrtive password to be changed. Therefore, as you setup these two cases make sure to create one for each. The names we use for these lambdas are:
mysql-rotation-administrator-lambda
and
mysql-rotation-nonAdministrator-lambda
And the names we use for these secrets are:
rds-cluster-administrator
and
rds-cluster-application

Select "Store a new secret"

Create an RS256 keys using:
ssh-keygen -t rsa -b 4096 -m PEM -f jwtRS256.key
# Don't add passphrase
openssl rsa -in jwtRS256.key -pubout -outform PEM -out jwtRS256.key.pub
and place into secrets manager as:
jwt-rs256
And put into fields "publicKey" and "privateKey".


Setup IAM User
--------------

For development and staging:
1) Open AWS IAM
2) Select "Groups"
3) Select "Create New Group"
4) Name it "developers" and select "Next Steps"
5) Filter and select "AdministratorAccess" then select "Next step" and "Create Group"
6) Select the Dashboard, Users, and "Add user"
7) Type in the developers.
8) Select both "Programmatic access" and "AWS Management Console access"
9) Select "Autogenerate password" and "Require password reset"
10) Using "Add users to group" add to the group created above.
11) Select "Next: Tags", "Next: Review", and "Create users".
12) MAKE SURE TO GET THE USER PASSWORDS AT THIS POINT! Then send the respective emails.

For production:
Give access only on a need to know basis.
Give full administrative rights.


Install Command Line Tools
--------------------------

Linux:

Install awscli:
yum install python-pip
pip install awscli

Install git:
yum -y install git

Macintosh:

Install git:
brew install git

Install AWS CLI tools: (see https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-macOS.html as of 03-Dec-2019 --tsp)
curl "https://d1vvhvl2y92vvt.cloudfront.net/awscli-exe-macos.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
rm awscliv2.zip

In each terminal window set $AWS_PROFILE to the development stage to connect to the different remote servers. Name the profiles:
export AWS_PROFILE=thelander-development
export AWS_PROFILE=thelander-staging
export AWS_PROFILE=thelander-production
The AWS_PROFILE will not be set in the ECS or Lambda contact so is used as a flag if debugging.

Install latest version of node.js:
node -v
npm cache clean -f
npm install -g n
n stable
node -v

Configure awscli to reference the correct account:
aws configure
Fill in the AWS Access Key and AWS Access Secret to those created above.
Set the default region to be that of the project likely: us-east-1
Default output format: json


Setup CDN Bucket
----------------

AWS CloudFront is the AWS Content Delivery Network (CDN) offering.

CloudFront is "backed" (obtains its contents) from and S3 bucket. AWS calls this the 'origin'.

The S3 bucket is manually created here because it is needed in the build step but the CloudFront distribution is created by CloudFormation.

1) Open AWS S3.
2) Select "Create bucket"
3) Bucket Name: {project name}-{development stage}-cloudformation-origin
4) Same region as the rest of the applciation.
5) Do not copy settings from another bucket.
6) Select "Next"
7) Do not version, no access log, no object level logging, no encryption, and no object locking.
8) Select default access setting recommended.
9) Select "Create bucket"

Record the CDN bucket name in the development, staging and production parameters files in the parameters/ directory.

Add the CORs:
<?xml version="1.0" encoding="UTF-8"?>
<CORSConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
<CORSRule>
    <AllowedOrigin>*</AllowedOrigin>
    <AllowedMethod>GET</AllowedMethod>
    <AllowedHeader>*</AllowedHeader>
</CORSRule>
</CORSConfiguration>


Setup HTTPS Certificate
-----------------------

1) Open AWS Certificate Manager
2) Select "Get Started" under the Provision certificates
3) Select "Request a public certificate"
4) Create the Domain name: {development stage}.{subdomain}.{base domain} In the case of the production it will be: {subdomain}.{base domain} and: www.{subdomain}.{base domain}
5) Select DNS validation, "Review", and then "Confirm and request".
6) Grab the ARN which will be needed for the HTTPSCertificateArn in the parameters JSON file.

NOTE: If you try to build the pipeline beofre the certificate is approved by adding the CNAME DNS record below it will report "certificate not found" since the certificate does not exist until approved.


Setup Fargate Application Cluster
---------------------------------

Move the files and sub-directories which are with these instructions into the root of your project. These files are templates and do not need to be modified:
                buildspec.yml
                cloudformation.template.yml
                pipeline.template.yml

This single index.html in a folder public/ allows the client application to be loaded over HTTPS by the nodejs application and the index.html should be modified to make calls to the CDN endpoint over HTTPS once the CDN endpoint is created below using the yml scripts listed above.
                public/

This folder includes the client application, will be pointed to by public/index.html and will have all relative paths. It will be put into the S3 bucket created above and moved by the build into the CloudFront CDN.
                client/

The parameter files should be modified to match the build context. The VPC and subdomains can be copied from AWS VPC. The ProjectName, BaseDomain, and Subdomain should be set. ReleaseImage should stay RELEASE_IMAGE_URL.
                parameters/

The application server code may have different structure and names but assuming it is in this form:
                node/
                node/server
                node/server/server.js

The Dockerfile should be set to move server/ into /app of the docker image.
                Dockerfile

The npm packages should be the minimum needed to run the appliation. To do this make sure to see what npm packages are installed globally:
npm list -g --depth 0
Reduce this as much as possible. Common to have mocha, chai, and others so they can be called from any level in the source tree.
Use:
npm install {package name}
called from the same directory which has:
                package.json
to install the packages needed by the application. The build will tell you if/when packages are missing if you have too few so start small and build up.

Run "npm instaall" and check in the 'package-lock.json' file. This file has the exact versions controlled by the developer so each time the docker images are recreated they do not automatically update all the npm packages and possibly destabilize the build.

.dockerignore will not exclude any files at this time.

.gitignore should exclude files and folders which are not those listed above.

Prepare the parameters used to name elements for the creation of the code pipeline. Here is an example:
StackName=tree-pipeline					// The name of the pipeline stack which is different from the cluster stack.
DevelopmentStage=DevelopmentStage		// Usually development, demonstration, staging, or production.
RepoProvider=GitHub 					// Instructions for GitHub.
RepoOwner=brads-qpi 					// The owner part of owner/project part of a source code repository.
RepoProject=qptree 						// The project part of owner/project part of a source code repository.
RepoBranch=master 						// The branch of a source code repository.
RepoToken={repo token} 					// GitHub access token generated above.

You will set {CDN URL} to 'tbd' and have to run and fail at leanst once so the ClientCDNDomainName is created.

Fill in the values above into the createPipeline.sh script.

The createPipeline.yml creates a pipeline which will be triggered when sources are checked into the source control branch for the stage of development.

You can see the cluster in the AWS console for the account going to AWS "ECS" and then selecting "Clusters".

You will also see the CodePipeline kicked off by going to AWS "CodePipeline", selecting Pipelines, and the pipeline you just created.

Once run obtain ALB IP using one of these methods and it will then be used in setting up the DNS record:
- AWS EC2, select Load Balancers, and select the load balancer you just created.
- Obtain the domain name for the outside interface of the load balancer.
- Because the domain name is a CNAME you can not make calls to curl using a CNAME so use dig to get the first IP address. Note, this address will rotate over time.
- Test access using:
curl -k https://ipFromAbove/health

Once run obtain the CDN End point from AWS CloudFront, replace TBD in the parameter:
ClientCDNDomainName=TBD
And run the command line again.


Setup DNS Records
-----------------

A CNAME will have to be created for the Secuity Certificate and the request to the IT person may look like this:

Diligent is being deployed as an AWS ECS Fargate Cluster behind an AWS Application Load Balancer (ALB).

The HTTPS certificates are handled by AWS because all traffic is forwarded to a cluster of redundant load balancers which then dispatch the requests to a cluster of application servers. AWS handles the certificate across the load balancer cluster.

At this time we are setting up just:
staging.diligent.powerstandards.com

Later, not now, we will follow a similar process for:
development.diligent.powerstandards.com
diligent.powerstandards.com
www.diligent.powerstandards.com

I see that the registrar is networksolutions.com

The WHOIS record:
https://www.networksolutions.com/whois/results.jsp?domain=powerstandards.com
Shows the name servers to be:
NS1.POWERSTANDARDS.COM
NS1.POWERSTANDARDS.COM

Which suggests the Primary DNS is Network Solutions.

We must create a CNAME or an A Record with an Alias. If you are using Route53 you must create an A record and make it of type “Alias”. Different DNS providers do it in different ways because the RFC specification technically says one cannot have a CNAME which also has an A Record. But, Network Solutions says to do it with a CNAME using these instructions:
http://www.networksolutions.com/support/cname-records-host-aliases/

NOTE: "Alias" in the next instruction is not part of the RFC standard and Amazons position is:
- ALIAS is not part of the RFC standard but when offered by a provider has the CNAME fully resolve to an IP address and not require the requesting service to do mulitple lookups.
- ALIAS allows one to create an A record but also have it be a redirection to deal with the apex record (the root domain abc.com) problem of CNAME by restriction of no other records; Meaning you can not have a CNAME of abc.com and an MX of abc.com.

Create a CNAME record:
_4b86e98258fce44c8b7471655a24ca22.staging.diligent.powerstandards.com.
With Value:
_aa17193b388abd32f5643b0cfe6d7844.hkvuiqjoua.acm-validations.aws.
NOTE: The dot at the end of the value must be preserved.

This CNAME record will allow AWS to confirm the person in control of the domain name has authorized the HTTPS certificate for staging.diligent.powerstandards.com which is in the record name. And, you can revoke the HTTPS certificate at any time by deleting the CNAME record.

Once you have made these changes let me know and I will test.

Additionally, an CNAME or A record alias must be created to point:
staging.diligent.powerstandards.com
To the outside address of the Application Load Balancer (ALB):
dualstack.
The "dualstack" tells the ALB it is IPv4 and IPv6 compatible.

Create a Route53 A record with "Alias Yes" to point the sub-domain {development stage}.{base domain name} with the value being the outside address of the ALB obtained above. If using Route53 select the "Routing Policy" as "Simple" and "Evaluate Target Health" as "No" since the ALB will do all the health testing and redirection. Add a period (.) to the end of the Alias. Add "dualstack." to the front of the Alias which tells Route53 to return IPv4 or IPv6 depending on the requester.

Once the ALB is created have the create a CNAME record which points {subdomain}.{domain name} to the endpoint for the ALB.Clou


Debug Cluster
-------------

Build Error
- Select the "Details" link on the left side of the build step in the pipeline view.
- This will show the build logs, phase details, and environment variables.
- Write to stdout in buildspec.yml will show the in the build logs.

Deployment Error
- Open AWS CloudWatch
- There will be a large number of log groups
- Look for the /ecs/ log group
- Most recent on the top.


Configure SES
-------------

1) Connect to Simple Email Service (SES) service using AWS console.
2) Connect to the Primary DNS and navigate to the hosted zone for the email domain to be configured.
3) Select "Verify a New Domain" button in SES.
4) Put in domain to verify and select "Generate DKIM Settings"
5) Please the keys into the DNS hosted zone as specified in the SES dialog.
6) SES shoulld show verification, DKIM, and enabled for sending verified.

If you want the email to be from a single domain company.com from different AWS accounts you:
1. In the Amazon Route 53 console, choose the _amazonses TXT record you added when you verified your domain in the first region.
2. In the Value box, press Enter after the first value.
3. Add the value for the additional region, and save the record set.

Per this article:
https://stackoverflow.com/questions/39654698/can-we-verify-or-have-same-domain-in-amazon-ses-from-different-aws-account

Ask to be removed from sandbox mode:
https://docs.aws.amazon.com/ses/latest/DeveloperGuide/request-production-access.html


Configure SNS
-------------

1) Connect to Simple Notification Service (SNS) service using AWS console.
2) Under "Mobile" select "Text Messaging".
3) Select "Edit" under "Text Messaging Preferences"
4) Fill out form:
a) Transactional
b) 1 (Can not be larger than authorized limit which is 1 by default)
c) 'ThelanderDv'
d) Bucket name webservices-{developmentStage}-thelander-data-sns-usage-reports
e) Delivery status logging of 100%
f) Create new role (Created two and filled in neither, hand put in ARN of failure case.)


Restore Thelander Platform from backup
--------------------------------------

Download analtyic database from thelander-analytics-backup
Download analtyic database from thelander-analytics-backup

Feed into:

gunzip < thelander_analytical_db_backup_latest.sql.gz | sed 's/\sDEFINER=`[^`]*`@`[^`]*`//g' | mysql -u admin -pPj94mP37xx -h production-cluster.cluster-cluddbkfqx68.us-east-1.rds.amazonaws.com

and

gunzip < thelander_transactional_db_backup_latest.sql.gz | sed 's/\sDEFINER=`[^`]*`@`[^`]*`//g' | mysql -u admin -pPj94mP37xx -h production-cluster.cluster-cluddbkfqx68.us-east-1.rds.amazonaws.com

Ignore the warning at the end of database restore about super privilages.

Apply all RX to analytics
load all functions

Apply all RX to transactional
load all functions

Copy files from development account jthelander-report-backup locally:
aws s3 sync s3://jthelander-report-backup .
Switch to the target account and use:
aws s3 sync . s3://webservices-staging-thelander-data/reports

The appUser password must be set on the new database using 
SET PASSWORD FOR 'user-name'@'localhost' = PASSWORD('NEW_USER_PASSWORD');
FLUSH PRIVILEGES;
Referenced in this article:
https://linuxize.com/post/how-to-change-mysql-user-password/


References
----------

Excellent series on many aspects including Aurora, Fargate,...
https://www.sourceallies.com/2018/07/node-reference-intro/

Install GIT on Mac:
https://sourceforge.net/projects/git-osx-installer/postdownload

Redirect the HTTP traffic to the HTTPS using the redirect functionality in the ALB which is described in:
https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html#listener-rules
The specification here:
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-elasticloadbalancingv2-listener.html
with Cloudformation YAML syntax here:
https://www.reddit.com/r/aws/comments/95i3b5/cloudformation_alb_redirect_support/
Additional syntax suggested here:
https://docs.aws.amazon.com/cli/latest/reference/elbv2/create-rule.html

Manual for CloudFormation:
http://docs.amazonaws.cn/en_us/AWSCloudFormation/latest/UserGuide/cfn-ug.pdf

Git Manual:
https://git-scm.com/docs/git-config

Setting Database Passwords will be done in the database itself not using IAM due to this limitation:
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html#UsingWithRDS.IAMDBAuth.Availability

AWS Manual CloudFormation:
https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-ug.pdf

AWS Manual CodePipeline:
https://docs.aws.amazon.com/codepipeline/latest/userguide/codepipeline-user.pdf

AWS Manual CodeBuild:
https://docs.aws.amazon.com/codebuild/latest/APIReference/codebuild-api.pdf

Node built in modules:
https://www.w3schools.com/nodejs/ref_modules.asp

AWS RDS Manual:
https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-ug.pdf